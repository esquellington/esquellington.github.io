#+STARTUP: indent overview

Literary programming exercise: test AVX vs SSE vs Scalar code

* Reference
- Run code block with C-c C-c
- Edit code with C-c ' in separate major-mode buffer (indirect, narrow)
- Tangle code with org-babel-tangle (req :tangle attrib in src block)
** org-babel intro https://orgmode.org/worg/org-contrib/babel/intro.html#source-code-blocks-org
** Supported languages https://orgmode.org/worg/org-contrib/babel/languages.html
** Tutorial http://howardism.org/Technical/Emacs/literate-programming-tutorial.html
** Another tutorial: https://caiorss.github.io/Emacs-Elisp-Programming/Org-mode-recipes.html#sec-1-5
** C++ execution commented https://www.cs.unm.edu/~eschulte/data/babel-c-execution.html
* PRELUDE: elisp CUSTOMIZE VAR to avoid repeated confirmation prompt
#+BEGIN_SRC elisp :results silent
  (setq org-confirm-babel-evaluate nil)
#+END_SRC
* DONE C++ TEST-AVX
** CODE
-O3 causes auto-vectorization and float1 is equivalent to simd8
#+name: test-avx
#+BEGIN_SRC C++ :tangle test-avx.cpp :flags -std=c++14 -O3 -march=native
  //See https://orgmode.org/worg/org-contrib/babel/languages/ob-doc-C.html
  #include <stdio.h>
  #include <chrono>
  #include <immintrin.h>
  using namespace std;
  int main()
  {
      const int cNumEntries(1024);
      const int cNumFloats(8*cNumEntries);
      const int cNumIter(100000);
      // Input
      float* vecWeights8( (float*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );
      float* vecCoords8( (float*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );
      for( int i=0; i<cNumFloats; ++i ) vecWeights8[i] = float(i+1)/cNumFloats;
      for( int i=0; i<cNumFloats; ++i ) vecCoords8[i] = cNumFloats/float(i+1);
      // Output
      float* vecResults( (float*)_mm_malloc( sizeof(float)*cNumEntries, 32 ) );

      // Tests
      auto s = chrono::system_clock::now();
      float acc_int1(0.0f);
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
          {
              int tmp(0);
              for( int k=0; k<8; ++k )
                  tmp += reinterpret_cast<const int*>(vecWeights8)[8*j+k]
                         ,* reinterpret_cast<const int*>(vecCoords8)[8*j+k];
              vecResults[j] = (float)tmp;
              acc_int1 += vecResults[j];
          }
      auto e = chrono::system_clock::now();
      printf("INT1 %d T %1.6f\n", (int)acc_int1, chrono::duration<float>(e-s).count());

      float acc_float1(0.0f);
      s = chrono::system_clock::now();
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
          {
              float tmp(0);
              for( int k=0; k<8; ++k )
                  tmp += vecWeights8[8*j+k] * vecCoords8[8*j+k];
              vecResults[j] = tmp;
              acc_float1 += vecResults[j];
          }
      e = chrono::system_clock::now();
      printf("FLOAT1 %f T %1.6f\n", acc_float1, chrono::duration<float>(e-s).count());

      float acc_simd4(0.0f);
      s = chrono::system_clock::now();
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
          {
              __m128 simd03 = _mm_mul_ps( _mm_load_ps(vecWeights8+j*8),
                                          _mm_load_ps(vecCoords8+j*8) );
              __m128 simd47 = _mm_mul_ps( _mm_load_ps(vecWeights8+j*8+4),
                                          _mm_load_ps(vecCoords8+j*8+4) );
              //TODO ensure this adds 4 lanes
              vecResults[j] = _mm_hadd_ps( simd03, _mm_hadd_ps( simd03, simd03 ) )[0]
                              + _mm_hadd_ps( simd47, _mm_hadd_ps( simd47, simd47 ) )[0];
              acc_simd4 += vecResults[j];
          }
      e = chrono::system_clock::now();
      printf("SIMD4 %f T %1.6f\n", acc_simd4, chrono::duration<float>(e-s).count());

      float acc_simd8(0.0f);
      s = chrono::system_clock::now();
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
          {
              __m256 simd07 = _mm256_mul_ps( _mm256_load_ps(vecWeights8+j*8),
                                             _mm256_load_ps(vecCoords8+j*8) );
              //TODO ensure this adds 8 lanes
              vecResults[j] = _mm256_hadd_ps(simd07,_mm256_hadd_ps(simd07,simd07) )[0]
                              + _mm256_hadd_ps(simd07,_mm256_hadd_ps(simd07,simd07) )[0];
              acc_simd8 += vecResults[j];
          }
      e = chrono::system_clock::now();
      printf("SIMD8 %f T %1.6f\n", acc_simd8, chrono::duration<float>(e-s).count());
      _mm_free( vecWeights8 );
      _mm_free( vecCoords8 );
      _mm_free( vecResults );
      return 0;
  }
#+END_SRC

#+RESULTS: test-avx

** elisp TANGLE to .cpp
#+BEGIN_SRC elisp :results silent
  (format "Calling tangle on test-avx")
  (org-babel-tangle)
#+END_SRC
** shell COMPILE to EXE and ASM
- Ideally, we'd get org-babel to yield intermediate asm and exe, but
  they seem to just be temporaries and not accessible, so we do it
  here explicitly
#+BEGIN_SRC sh :results silent
  echo "Compiling tangled text-avx.cpp"
  g++ -std=c++14 -O3 -march=native test-avx.cpp -o test-avx
  g++ -std=c++14 -O3 -march=native test-avx.cpp -S -o test-avx.S
#+END_SRC
** shell RUN
#+BEGIN_SRC sh
  echo "Running text-avx"
  ./test-avx
#+END_SRC

#+RESULTS:

** TODO gnuplot PLOT RESULTS
- Use C-c C-x C-v to display inline image

*** Plot function
#+BEGIN_SRC gnuplot :file function.png
  plot sin(x)
#+END_SRC

#+RESULTS:
[[file:function.png]]
*** Plot table
#+tblname: data-table
| x | y1 | y2 |
|---+----+----|
| 0 |  0 |  0 |
| 1 |  1 |  1 |
| 2 |  4 |  8 |
| 3 |  9 | 27 |

#+BEGIN_SRC gnuplot :var data=data-table :file table.png
  plot data u 1:2 with lines, \
       data u 1:3 with lines
#+END_SRC

#+RESULTS:
[[file:table.png]]

* DONE C++ TEST-AVX-GATHER
** CODE
-O3 causes auto-vectorization and float1 is equivalent to simd8
#+name: test-avx-gather
#+BEGIN_SRC C++ :tangle test-avx-gather.cpp :flags -std=c++14 -O3 -march=native
  //See https://orgmode.org/worg/org-contrib/babel/languages/ob-doc-C.html
  #include <stdio.h>
  #include <chrono>
  #include <immintrin.h>
  using namespace std;
  // Computes same weighted sum as TEST-AVX, but on indexed coords
  int main()
  {
      const int cNumEntries(1024);
      const int cNumFloats(8*cNumEntries);
      const int cNumIter(100000);
      // Input
      float* vecCoords8( (float*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );
      float* vecWeights8( (float*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );
      int* vecIndices8( (int*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );
      for( int i=0; i<cNumFloats; ++i ) vecCoords8[i] = cNumFloats/float(i+1);
      for( int i=0; i<cNumFloats; ++i ) vecWeights8[i] = float(i+1)/cNumFloats;
      for( int i=0; i<cNumFloats; ++i ) vecIndices8[i] = cNumFloats - i - 1;
      // Output
      float* vecResults8( (float*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );
      // Timers
      auto s = chrono::system_clock::now();
      auto e = chrono::system_clock::now();
      // Tests
      s = chrono::system_clock::now();
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
              for( int k=0; k<8; ++k )
                  vecResults8[8*j+k] = vecWeights8[8*j+k] * vecCoords8[ vecIndices8[8*j+k] ];
      e = chrono::system_clock::now();
      float acc_float1(0.0f);
      for( int i=0; i<cNumFloats; ++i ) acc_float1 += vecResults8[i];
      printf("FLOAT1 %f T %1.6f\n", acc_float1, chrono::duration<float>(e-s).count());

      s = chrono::system_clock::now();
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
          {
              // AVX2 aligned load
              __m128i indices03 = _mm_load_si128( (const __m128i*)(vecIndices8+j*8) );
              __m128i indices47 = _mm_load_si128( (const __m128i*)(vecIndices8+j*8+4) );
              __m128 gather03 = _mm_i32gather_ps( vecCoords8, indices03, 4 );
              __m128 gather47 = _mm_i32gather_ps( vecCoords8, indices47, 4 );
              __m128 simd03 = _mm_mul_ps( _mm_load_ps(vecWeights8+j*8), gather03 );
              __m128 simd47 = _mm_mul_ps( _mm_load_ps(vecWeights8+j*8+4), gather47 );
              _mm_store_ps( vecResults8+j*8, simd03 );
              _mm_store_ps( vecResults8+j*8+4, simd47 );
          }
      e = chrono::system_clock::now();
      float acc_simd4(0.0f);
      for( int i=0; i<cNumFloats; ++i ) acc_simd4 += vecResults8[i];
      printf("SIMD4 %f T %1.6f\n", acc_simd4, chrono::duration<float>(e-s).count());

      s = chrono::system_clock::now();
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
          {
              // AVX2 aligned load
              //TODO TRY MULTIPLE GATHERS in seq to fake getting consecutive x,y,z
              __m256i indices07 = _mm256_load_si256( (const __m256i*)(vecIndices8+j*8) );
              __m256 gather07 = _mm256_i32gather_ps( vecCoords8, indices07, 4 );
              __m256 simd07 = _mm256_mul_ps( _mm256_load_ps(vecWeights8+j*8), gather07 );
              _mm256_store_ps( vecResults8+j*8, simd07 );
          }
      e = chrono::system_clock::now();
      float acc_simd8(0.0f);
      for( int i=0; i<cNumFloats; ++i ) acc_simd8 += vecResults8[i];
      printf("SIMD8 %f T %1.6f\n", acc_simd8, chrono::duration<float>(e-s).count());

      _mm_free( vecCoords8 );
      _mm_free( vecWeights8 );
      _mm_free( vecIndices8 );
      _mm_free( vecResults8 );
      return 0;
  }
#+END_SRC

#+RESULTS: test-avx-gather
| FLOAT1 | 70364.03125 | T | 0.830171 |
| SIMD4  | 70364.03125 | T | 1.142072 |
| SIMD8  | 70364.03125 | T | 0.873745 |

** elisp TANGLE to .cpp
#+BEGIN_SRC elisp :results silent
  (format "Calling tangle on test-avx-gather")
  (org-babel-tangle)
#+END_SRC
** shell COMPILE to EXE and ASM
- Ideally, we'd get org-babel to yield intermediate asm and exe, but
  they seem to just be temporaries and not accessible, so we do it
  here explicitly
#+BEGIN_SRC sh :results silent
  echo "Compiling tangled text-avx-gather.cpp"
  g++ -std=c++14 -O3 -march=native test-avx-gather.cpp -o test-avx-gather
  g++ -std=c++14 -O3 -march=native test-avx-gather.cpp -S -o test-avx-gather.S
#+END_SRC
** shell RUN
#+BEGIN_SRC sh
  echo "Running text-avx-gather"
  ./test-avx-gather
#+END_SRC

#+RESULTS:
| Running | text-avx-gather |   |          |
| INT1    |     -2147483648 | T | 0.348499 |
| FLOAT1  |     134217728.0 | T | 0.384117 |
| SIMD4   |      67108864.0 | T | 0.275123 |
| SIMD8   |      67108864.0 | T | 0.203366 |

* DONE C++ TEST-AVX-GATHER-Vec3f
** CODE
o-O3 causes auto-vectorization and float1 is equivalent to simd8
#+name: test-avx-gather-vec3f
#+BEGIN_SRC C++ :tangle test-avx-gather-vec3f.cpp :flags -std=c++14 -O3 -march=native
  //See https://orgmode.org/worg/org-contrib/babel/languages/ob-doc-C.html
  #include <stdio.h>
  #include <chrono>
  #include <immintrin.h>
  using namespace std;

  const int cNumIter(10000);
  const int cNumEntries(1000);
  const int cNumFloats(8*cNumEntries);

  void FLOAT1( float* vecResults8, const float* vecCoords3x8, const float* vecWeights8, const int* vecIndices8 )
  {
      auto s = chrono::system_clock::now();
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
              for( int k=0; k<8; ++k )
              {
                  const int base_idx( 3*vecIndices8[8*j+k] );
                  const float w( vecWeights8[8*j+k] );
                  vecResults8[8*j+k]  = w * vecCoords3x8[ base_idx + 0 ];
                  vecResults8[8*j+k] += w * vecCoords3x8[ base_idx + 1 ];
                  vecResults8[8*j+k] += w * vecCoords3x8[ base_idx + 2 ];
              }
      auto e = chrono::system_clock::now();
      float acc_float1(0.0f);
      for( int i=0; i<cNumFloats; ++i ) acc_float1 += vecResults8[i];
      printf("FLOAT1 %f T %1.6f\n", acc_float1, chrono::duration<float>(e-s).count());
  }

  void SIMD8( float* vecResults8, const float* vecCoords3x8, const float* vecWeights8, const int* vecIndices8 )
  {
      auto s = chrono::system_clock::now();
      const __m256i one07( _mm256_set1_epi32(1) );
      const __m256i three07( _mm256_set1_epi32(3) );
      for( int i=0; i<cNumIter; ++i )
          for( int j=0; j<cNumEntries; ++j )
          {
              // Load base indices and multiply by stride 3
              // IMPORTANT mullo does C07=A07*B07, regular mul just multiplies 0..3!!
              __m256i base07 = _mm256_load_si256( (const __m256i*)(vecIndices8+j*8) );
              base07 = _mm256_mullo_epi32( base07, three07 );
              // Get (X,Y,Z) by incrementing indices
              __m256 x07 = _mm256_i32gather_ps( vecCoords3x8, base07, 4 );
              __m256i indices07 = _mm256_add_epi32( base07, one07 );
              __m256 y07 = _mm256_i32gather_ps( vecCoords3x8, indices07, 4 );
              indices07 = _mm256_add_epi32( indices07, one07 );
              __m256 z07 = _mm256_i32gather_ps( vecCoords3x8, indices07, 4 );
              // Get weights
              __m256 w07 = _mm256_load_ps(vecWeights8+j*8);
              // w*(X,Y,Z)
              __m256 wx07 = _mm256_mul_ps( w07 , x07 );
              __m256 wy07 = _mm256_mul_ps( w07, y07 );
              __m256 wz07 = _mm256_mul_ps( w07, z07 );
              // Store wX+wY+wZ
              _mm256_store_ps( vecResults8+j*8,
                               _mm256_add_ps( wx07, _mm256_add_ps( wy07, wz07 ) ) );
          }
      auto e = chrono::system_clock::now();
      float acc_simd8(0.0f);
      for( int i=0; i<cNumFloats; ++i ) acc_simd8 += vecResults8[i];
      printf("SIMD8 %f T %1.6f\n", acc_simd8, chrono::duration<float>(e-s).count());

  }

  // Computes same weighted sum as TEST-AVX, but on indexed coords
  int main()
  {
      // Input
      // coords xyz,xyz,...
      float* vecCoords3x8( (float*)_mm_malloc( 3*sizeof(float)*cNumFloats, 32 ) );
      float* vecWeights8( (float*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );
      int* vecIndices8( (int*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );
      for( int i=0; i<3*cNumFloats; ++i ) vecCoords3x8[i] = cNumFloats/float(i+1);
      for( int i=0; i<cNumFloats; ++i ) vecWeights8[i] = float(i+1)/cNumFloats;
      for( int i=0; i<cNumFloats; ++i ) vecIndices8[i] = cNumFloats - i - 1;
      // Output
      float* vecResults8( (float*)_mm_malloc( sizeof(float)*cNumFloats, 32 ) );

      FLOAT1(vecResults8,vecCoords3x8,vecWeights8,vecIndices8);
      SIMD8(vecResults8,vecCoords3x8,vecWeights8,vecIndices8);

      _mm_free( vecCoords3x8 );
      _mm_free( vecWeights8 );
      _mm_free( vecIndices8 );
      _mm_free( vecResults8 );
      return 0;
  }
#+END_SRC

#+RESULTS: test-avx-gather-vec3f

** elisp TANGLE to .cpp
#+BEGIN_SRC elisp :results silent
  (format "Calling tangle on test-avx-gather-vec3f")
  (org-babel-tangle)
#+END_SRC
** shell COMPILE to EXE and ASM
- Ideally, we'd get org-babel to yield intermediate asm and exe, but
  they seem to just be temporaries and not accessible, so we do it
  here explicitly
#+BEGIN_SRC sh :results silent
  echo "Compiling tangled text-avx-gather-vec3f.cpp"
  g++ -std=c++14 -O3 -march=native test-avx-gather-vec3f.cpp -o test-avx-gather-vec3f
  g++ -std=c++14 -O3 -march=native test-avx-gather-vec3f.cpp -S -o test-avx-gather-vec3f.S
#+END_SRC
** shell RUN
#+BEGIN_SRC sh
  echo "Running text-avx-gather-vec3f"
  ./test-avx-gather-vec3f
#+END_SRC

#+RESULTS:

** Analysis
- At -O3 -mavx2, FLOAT1 uses 128b instructions (xmm) and runs 10%
  faster than SIMD8.
- Looking at the assembly, FLOAT1 seems to unroll the loop heavily (8
  iter?), while SIMD8 is almost a direct translation of the intrinsic code.
* DONE C++ Pointer aliasing/restrict
- Check HOW array-processing funcs like Combine( int count, float* dst,
  const float* src1, const float* src2 ) that have guaranteed
  no-aliasing and benefit from __restrict__
- Compare with same Combine() when receiving std::vector or std::span
  and see if they can be restricted in any way to yield the same
  assembly
- This article is interesting https://travisdowns.github.io/blog/2019/08/26/vector-inc.html
** CODE
#+name: test-restrict
#+BEGIN_SRC C++ :tangle test-restrict.cpp :flags -std=c++14 -O3 -march=native
  #include <stdio.h>
  #include <chrono>
  #include <vector>
  using namespace std;
  const int cNumEntries(100000000);
  __attribute__ ((noinline))
  void Lerp_Vector( std::vector<float>& dst, float lambda01, const std::vector<float>& src1, const std::vector<float>& src2 )
  {
      const float w1( 1.0f - lambda01 ); const float w2( lambda01 ); const int count( dst.size() );
      auto s = chrono::system_clock::now();
      for( int i=0; i<count; ++i )
      {
          dst[i] = w1*src1[i] + w2*src2[count-1-i];
          dst[i] += src1[i] + src2[i];
          dst[i] -= src1[count-1-i/2] + src2[count-1-i/2];
      }
      auto e = chrono::system_clock::now();
      float acc(0.0f); for( auto v : dst ) acc += v;
      printf( "LerpV  %1.6f %e\n", chrono::duration<float>(e-s).count(), acc);
  }
  __attribute__ ((noinline))
  void Lerp_NoRestrict( float* dst, int count, float lambda01, const float* src1, const float* src2 )
  {
      const float w1( 1.0f - lambda01 ); const float w2( lambda01 );
      auto s = chrono::system_clock::now();
      for( int i=0; i<count; ++i )
      {
          dst[i] = w1*src1[i] + w2*src2[count-1-i];
          dst[i] += src1[i] + src2[i];
          dst[i] -= src1[count-1-i/2] + src2[count-1-i/2];
      }
      auto e = chrono::system_clock::now();
      float acc(0.0f); for( int i=0; i<count; ++i ) acc += dst[i];
      printf( "LerpNR %1.6f %e\n", chrono::duration<float>(e-s).count(), acc);
  }
  __attribute__ ((noinline))
  void Lerp_Restrict( float* __restrict__ dst, int count, float lambda01, const float* __restrict__ src1, const float* __restrict__ src2 )
  {
      const float w1( 1.0f - lambda01 ); const float w2( lambda01 );
      auto s = chrono::system_clock::now();
      for( int i=0; i<count; ++i )
      {
          dst[i] = w1*src1[i] + w2*src2[count-1-i];
          dst[i] += src1[i] + src2[i];
          dst[i] -= src1[count-1-i/2] + src2[count-1-i/2];
      }
      auto e = chrono::system_clock::now();
      float acc(0.0f); for( int i=0; i<count; ++i ) acc += dst[i];
      printf( "LerpR  %1.6f %e\n", chrono::duration<float>(e-s).count(), acc);
  }
  int main()
  {
      // Alloc and init/warmup all memory
      std::vector<float> src1( cNumEntries, 0.0f );
      std::vector<float> src2( cNumEntries, 1.0f );
      std::vector<float> dst( cNumEntries, 0.5f );
      // Profile with no aliasing
      Lerp_Vector( dst, 0.5f, src1, src2 );
      Lerp_NoRestrict( &dst[0], cNumEntries, 0.5f, &src1[0], &src2[0] );
      Lerp_Restrict( &dst[0], cNumEntries, 0.5f, &src1[0], &src2[0] );
      // Profile with aliasing
      for( int i=0; i<cNumEntries; ++i ) dst[i] = float(i)/(cNumEntries-1);
      Lerp_Vector( dst, 0.33f, dst, dst );
      for( int i=0; i<cNumEntries; ++i ) dst[i] = float(i)/(cNumEntries-1);
      Lerp_NoRestrict( &dst[0], cNumEntries, 0.33f, &dst[0], &dst[0] );
      for( int i=0; i<cNumEntries; ++i ) dst[i] = float(i)/(cNumEntries-1);
      Lerp_Restrict( &dst[0], cNumEntries, 0.33f, &dst[0], &dst[0] );
      return 0;
  }
#+END_SRC

#+RESULTS: test-restrict
| LerpV  | 0.455969 | 8.388608e+06 |
| LerpNR | 0.462095 | 8.388608e+06 |
| LerpR  | 0.256006 | 8.388608e+06 |
| LerpV  | 0.371413 | 3.359361e+07 |
| LerpNR | 0.372052 | 3.359361e+07 |
| LerpR  | 0.249513 | 3.098080e+07 |
** Analysis
- These results (unplugged) show that LerpR is 50% faster BUT yield
  incorrect results when wrongly called on repeated input/output
  pointers, while std::vector LerpV and non-restricted LerpNR ptr
  versions are essentially equivalent
- Speedup surely depends on the amount of work done inside the loop,
  but this is quite significant, so the answer is RESTRICT ALL THE THINGS
RESULTS: unplugged
| LerpV  | 0.389423 | 8.388608e+06 |
| LerpNR | 0.387801 | 8.388608e+06 |
| LerpR  | 0.255761 | 8.388608e+06 |
| LerpV  | 0.369636 | 3.359361e+07 |
| LerpNR | 0.371537 | 3.359361e+07 |
| LerpR  | 0.248428 | 3.098080e+07 |

RESULTS: plugged
| LerpV  | 0.328719 | 8.388608e+06 |
| LerpNR | 0.322881 | 8.388608e+06 |
| LerpR  | 0.210487 | 8.388608e+06 |
| LerpV  |  0.25215 | 3.359361e+07 |
| LerpNR | 0.251189 | 3.359361e+07 |
| LerpR  | 0.186163 | 3.098080e+07 |
